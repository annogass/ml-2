IEEE-CIS Fraud Detection
კონკურსის მიზანია შექმნას ისეთი მოდელები, რომლებიც მაქსიმალურად დაადასტურებენ მაღალი ხარისხის მცდარ ტრანზაქციებს და უზრუნველყოფენ უფრო კარგ ბიზნეს გადაწყვეტილებებს საფინანსო სექტორისთვის.

ამ პროექტზე მუშაობისას პირველ რიგში დავიწყე თრეინ და ტესტ სეტების ამოღება და მათი დამერჯვა. შემდეგ კი თრეინის ნაწილი კიდევ ორ ნაწილად გავყავი 'isFraud' სვეტის  გარეშე და მხოლოდ ამ სვეტით. შემდეგ დავიწყე მონაცემების გასუფთავება nan მნიშვნელობებისგან და მათი ჩანაცვლება იმის შესაბამისად numerical იყო ამ სვეტის მნიშვნელობა თუ იყო object. ისეთი სვეტები რომლებშიც nan მნიშვნელობები 80% ს აღემატება დავდროპე რადგან მათზე მუსაობა არ იქნებოდა ეფექტური. შემდეგ შევქმენი CorrelationFilter და VarianceFilter კლასები ტრეინ სეტის გაუმჯობესებისთვის და მეტი ეფექტურობისთვის. ბოლოს კი კატეგორიული ცვლადები გადავიყვანე. ამის შემდეგ უკვე დავიწყე ფაიფლაინის აწყობა სხვადასხვა მოდელებიტ და ტრენინგი. ბოლო ნაწილში კი mlflow ზე ვტვირტავ ჩემს მიმდინარე ექსპერიმენტს.


რეპოზიტორიის სტრუქტურა:

model_experiment_Random Forest-მიმოხილვა:
ამ ნოუთბუქში ვატარებ ექსპერიმენტს Fraud Detection ამოცანაზე, სადაც მიზანია თაღლითური ტრანზაქციების ეფექტურად აღმოჩენა დიდი მოცულობის მონაცემებიდან. ძირითადად ფოკუსირებული ვარ Random Forest მოდელზე, თუმცა მთლიანი პროცესის აგება ისე მოხდილია, რომ შესაძლებელი იყოს სხვადასხვა ალგორითმების სწრაფი გამოცდა.
მიდგომები და ეტაპები:
Preprocessing & Feature Engineering:
მონაცემების გაწმენდა დავიწყე იმით, რომ მოვაშორე სვეტები, რომელთაც გამოტოვებული მნიშვნელობები ძალიან დიდი რაოდენობით ჰქონდათ. დანარჩენ ცვლადებზე გამოტოვებული მნიშვნელობები შევავსე — რიცხვითებზე ნულით, ხოლო კატეგორიულ ცვლადებზე "NO" მნიშვნელობით. რიცხვითი სვეტების მასშტაბირება გავაკეთე StandardScaler-ის გამოყენებით. კატეგორიული სვეტებისთვის გამოვიყენე TargetEncoder.
Feature Selection:
სანამ მოდელის სწავლებას დავიწყებდი, მოვახდინე ფიჩერების შერჩევა ორ ეტაპად:
მოვაშორე დაბალი დისპერსიის მქონე სვეტები VarianceFilter-ის გამოყენებით (threshold=0.01).
ზედმეტად კორელირებული სვეტები გამოვფილტრე CorrelationFilter-ის საშუალებით (threshold=0.95).
Pipeline აწყობა:
Pipeline ავაგე sklearn-ის კომპონენტების გამოყენებით. Pipeline მოიცავს როგორც preprocessing-ს, ასევე ფიჩერების შერჩევას და Random Forest მოდელს. დამატებით ფიჩერების შერჩევა გავაკეთე SelectFromModel-ის საშუალებით, რათა საბოლოოდ 250-მდე ფიჩერი შემენარჩუნებინა.
Model Training:
მონაცემები გავყავი train და validation სეტებად 80/20 პროპორციით, სტრატიფიკაციის და random_state=42-ის გათვალისწინებით. Pipeline ავარჯიშე train სეტზე და ვალიდაციაზე შევაფასე მოდელის ხარისხი ROC AUC მაჩვენებლით.
გამოყენებული მოდელი:
Random Forest Classifier
Random Forest-ის ჰიპერპარამეტრები:
n_estimators=300 (300 decision tree თითო Random Forest-ში)
max_depth=10 (მაქსიმალური სიღრმე თითოეული decision tree-სთვის)
min_samples_split=5 (წ mínima სემპლი, რომ განყოფა მოხდეს)
random_state=42 (გამეორებადობის უზრუნველსაყოფად)
შეფასება: ვალიდაციის შედეგების შეფასება ვასრულებ ROC AUC სკორით, რადგან კლასები გაუწონასწორებელია და ეს მეტრიკა ყველაზე უკეთ ასახავს მოდელის უნარს თაღლითური ტრანზაქციების აღმოჩენაში.



model_experiment_LogisticRegression-მიმოხილვა:
ამ ნოუთბუქში ექსპერიმენტს ვატარებ Fraud Detection ამოცანაზე, სადაც მთავარ მიზანს წარმოადგენს თაღლითური ტრანზაქციების აღმოჩენა მონაცემთა დიდი ნაკრებიდან. ამ ნოუთბუქში ვფოკუსირდი ძირითადად Logistic Regression მოდელზე, თუმცა მთლიან პროცესს ისე ვაწყობ, რომ შესაძლებელი იყოს სხვადასხვა მოდელის მარტივად გამოცდა.
მიმიდგომები და ეტაპები:
Preprocessing & Feature Engineering:ვწმენდ მონაცემებს, ვაშორებ ძალიან ბევრი გამოტოვებული მნიშვნელობის მქონე სვეტებს. ვასრულებ გამოტოვებული მნიშვნელობების შევსებას (ნულით რიცხვებისთვის, და "NO" მნიშვნელობით კატეგორიული ცვლადებისთვის). რიცხვით სვეტებს ვაპატარავებ StandardScaler-ით. კატეგორიულ სვეტებზე ვიყენებ TargetEncoder-ს.
Feature Selection:
დაბალი დისპერსიის მქონე სვეტებს ვაშორებ (VarianceFilter, threshold=0.01). ზედმეტად მაღალ კორელირებულ სვეტებს ვფილტრავ (CorrelationFilter, threshold=0.95).
Pipeline აწყობა:
ვაგენერირებ sklearn-ის Pipeline-ს, რომელიც მოიცავს preprocessing-ს, feature selection-ს და მოდელს.
Pipeline-ში ფიჩერების დამატებით შერჩევას ვასრულებ SelectFromModel-ის საშუალებით, სადაც ვუტოვებ მაქსიმუმ 250 ფიჩერს.
Model Training:
მონაცემებს ვყოფ train და validation სეტებად (80/20), სტრატიფიკაციით და random_state=42-ით. Pipeline-ს ვასწავლი train სეტზე. ვალიდაციაზე პროგნოზის შედეგად ვთვლი ROC AUC სკორს.
გამოყენებული მოდელი:
Logistic Regression (L2 რეგულარიზაციით)
Logistic Regression-ის ჰიპერპარამეტრები:
penalty='l2' (Ridge რეგულარიზაცია)
C=0.5 (რეგულარიზაციის სიძლიერე; ნაკლები მნიშვნელობა — მეტი რეგულარიზაცია)
solver='lbfgs' (სწრაფი ოპტიმიზატორი დიდი მონაცემებისთვის)
max_iter=1000 (სრულ კონვერგენციამდე მეტი ტრენინგი)
random_state=42 (გამეორებადობისთვის)
შეფასება: ვალიდაციის სკორინგი ხდება ROC AUC მაჩვენებლით, რომელიც ყველაზე ოპტიმალური მეტრიკაა კლასების გაუწონასწორებლობის შემთხვევებში.



model_experiment_XGBoost- მიმოხილვა:
ამ ნოუთბუქში კვლავ Fraud Detection ამოცანაზე ვმუშაობ, სადაც მიზანია დიდი მონაცემთა ნაკრებიდან თაღლითური ტრანზაქციების იდენტიფიცირება. მთლიან პროცესს ვაწყობ ისე, რომ სხვადასხვა მოდელის გამოცდა იყოს შესაძლებელი, თუმცა ამ ეტაპზე ყურადღება გამახვილებულია XGBoost მოდელზე.
მიდგომები და ეტაპები:
Preprocessing & Feature Engineering:
მონაცემების გაწმენდა და ფიჩერების ინჟინერია განხორციელდა სტანდარტული პროცედურებით: სვეტების მოცილება, სადაც ბევრი გამოტოვებული მნიშვნელობა იყო; დანარჩენების შევსება ნულით (რიცხვითებისთვის) და "NO"-თ (კატეგორიული სვეტებისთვის); რიცხვითი სვეტების სტანდარტიზაცია StandardScaler-ით და კატეგორიულ ცვლადებზე TargetEncoder-ის გამოყენება.
Feature Selection:
ფიჩერების შერჩევა VarianceFilter-ის და CorrelationFilter-ის გამოყენებით:
VarianceFilter (threshold=0.01) — დაბალი დისპერსიის მქონე სვეტების მოცილება
CorrelationFilter (threshold=0.95) — ზედმეტად კორელირებული სვეტების მოცილება
Pipeline აწყობა:
Pipeline მოიცავს preprocessing-ს, feature selection-ს და XGBoost Classifier-ს. დამატებით, SelectFromModel-ის მეშვეობით ვტოვებ მაქსიმუმ 250 ფიჩერს. ასეთი აგებულებით ვადვილებ მოდელის სწრაფ და მოქნილ შეფასებას სხვადასხვა მონაცემებზე.
Model Training:
მონაცემები გავყავი train და validation სეტებად 80/20 პროპორციით, სტრატიფიკაციის გათვალისწინებით. Pipeline ვატრენერე train სეტზე, ხოლო მოდელის შეფასება ვასრულე validation სეტზე ROC AUC სკორით, რაც ოპტიმალურია გაუწონასწორებელი კლასების შემთხვევაში.
გამოყენებული მოდელი:
XGBoost Classifier
XGBoost-ის ჰიპერპარამეტრები:
n_estimators=1200 (მოდელში 1200 decision tree-ის გამოყენება)
learning_rate=0.05 (დაბალი სასწავლო სიჩქარე, რაც ზრდის მოდელის სტაბილურობას)
max_depth=7 (თითოეული decision tree-ის მაქსიმალური სიღრმე)
use_label_encoder=False (XGBoost-ის ახალ ვერსიებში label encoder-ის გამორთვა)
eval_metric='logloss' (სწავლისას გამოყენებული სავალიდაციო მეტრიკა: logloss)
n_jobs=-1 (ყველა პროცესორის ბირთვის მაქსიმალური გამოყენება პარალელური გამოთვლებისთვის)
random_state=42 (გამეორებადობის უზრუნველყოფა)
შეფასება:
მოდელის შეფასება ვასრულებ ROC AUC მეტრიკით ვალიდაციის სეტზე, რაც ყველაზე შესაფერისია თაღლითური ტრანზაქციების იდენტიფიცირების ამოცანაში.


model_experiment_AdaBoost-ამ ნოუთბუქში ვაგრძელებ მუშაობას Fraud Detection ამოცანაზე, რომლის მიზანიც არის დიდი მოცულობის მონაცემებიდან თაღლითური ტრანზაქციების იდენტიფიცირება. ამ ეტაპზე ფოკუსირება გავაკეთე AdaBoostClassifier მოდელზე, რათა გამომეცადა მისი ეფექტურობა აღნიშნულ ამოცანაზე.
პირველ რიგში, განვახორციელე მონაცემთა გაწმენდა და ფიჩერების ინჟინერია. მოვიცილე ის სვეტები, სადაც გამოტოვებული მნიშვნელობების წილი მაღალი იყო, ხოლო დარჩენილი გამოტოვებული მნიშვნელობები შევავსე შესაბამისი წესით: რიცხვითი ცვლადები ნულით, ხოლო კატეგორიული ცვლადები "NO" მნიშვნელობით. რიცხვით სვეტებზე გამოვიყენე StandardScaler სტანდარტიზაციისათვის, ხოლო კატეგორიულ ცვლადებზე — TargetEncoder იმისათვის, რომ მოდელს უკეთ შეესწავლა კატეგორიული ინფორმაცია.
ფიჩერების შერჩევისთვის გამოვიყენე ორი ეტაპიანი მეთოდი: ჯერ დაბალი დისპერსიის მქონე სვეტები მოვიცილე VarianceFilter-ის (threshold=0.01) გამოყენებით, შემდეგ ზედმეტად კორელირებული სვეტები მოვაშორე CorrelationFilter-ის (threshold=0.95) დახმარებით. საბოლოოდ, SelectFromModel-ის საშუალებით ავარჩიე მაქსიმუმ 250 ფიჩერი, რაც ხელს უწყობს მოდელის ოპტიმალურ მუშაობას.
მთლიანი პროცესი ავაწყვე Pipeline-ის სახით, სადაც გაერთიანებულია მონაცემების პრეპროცესინგი, ფიჩერების შერჩევა და საბოლოო კლასიფიკაციის მოდელი. ასეთი სტრუქტურა მეხმარება პროცესის ავტომატიზაციას და სხვადასხვა მონაცემზე მარტივ აპლიკაციას.
მოდელის დასატრენინგებლად მონაცემები გავყავი სასწავლო და ვალიდაციის სეტებად 80/20 პროპორციით, სტრატიფიკაციის გათვალისწინებით, რათა ორივე სეტში კლასების ბალანსი შენარჩუნებულიყო. მოდელი ავატრენინგე სასწავლო სეტზე, ხოლო შეფასება ჩავატარე ვალიდაციის სეტზე ROC AUC მეტრიკის გამოყენებით, რომელიც ოპტიმალურია გაუწონასწორებელი კლასების მქონე ამოცანებში.
AdaBoostClassifier-ის კონფიგურაციისთვის შევარჩიე შემდეგი ჰიპერპარამეტრები: გამოიყენება DecisionTreeClassifier საბაზისო ესტიმატორად (max_depth=1), რაც უზრუნველყოფს მარტივი და სწრაფი გადაწყვეტილებების მიღებას. გამოიყენება 100 ესტიმატორი (n_estimators=100), სტანდარტული სასწავლო სიჩქარით (learning_rate=1.0), "SAMME.R" ალგორითმით, რომელიც დაფუძნებულია predict_proba-ზე და, შესაბამისად, უკეთ მუშაობს პრობაბილისტური პროგნოზებით. პროცესის გამეორებადობის უზრუნველსაყოფად დავაფიქსირე random_state=42.
მოდელის შედეგების და ექსპერიმენტების მენეჯმენტისთვის გამოვიყენე MLflow, სადაც ლოგავს როგორც ჰიპერპარამეტრებს, ასევე შეფასების მეტრიკებსა და სრულ Pipeline მოდელს. ეს მაძლევს საშუალებას შევადარო სხვადასხვა ვერსიის შედეგები და გამოვავლინო საუკეთესო კონფიგურაცია.



model_inference.ipynb-ამ ნოუთბუქში ხდება ჩემი საუკეთესო მოდელის დალოუდება და გატესტვა. საბმიშენს ვტვირთავ kaggle-ზე.

Feature Engineering:პირველ რიგში, ვქმნი სპეციალურ კლასი-ს, რომელსაც ვეძახი CustomNullCleaner.ეს კლასი მეხმარება მონაცემების გაწმენდაში და დაყრდნობილია რამდენიმე მნიშვნელოვან სტრატეგიაზე:მაღალი დანაკარგის მქონე სვეტების წაშლა:threshold პარამეტრის მიხედვით (default=0.8), ვპოულობ იმ სვეტებს, სადაც null-ის პროცენტი მეტია ან ტოლია 80%-ზე. ასეთ სვეტებს ვაგროვებ სიაში და შემდეგ ვშლი. ნარჩენი ნულოვანი მნიშვნელობების შევსება: რიცხვით სვეტებში (int64, float64 ტიპის) ვავსებ ყველა null მნიშვნელობას 0-ით. კატეგორიულ სვეტებში (object ტიპის) ვავსებ ყველა null მნიშვნელობას ტექსტით "NO". ამის შემდეგ მონაცემები გაცილებით სუფთა და დამუშავებულია მოდელისთვის მოსამზადებლად.CustomNullCleaner-ის გამოყენების შემდეგ გაწმენდილ მონაცემებში ვადგენ, რომელი სვეტებია:კატეგორიული (dtype=object) რიცხვითი (dtype=int64 ან float64)
ეს დამახასიათებელია იმისთვის, რომ შემდგომი წინამოწესებული დამუშავება სწორად დავგეგმო სხვადასხვა ტიპის მონაცემებისთვის.შემდეგ ვაგენერირებ preprocessing pipelines:
რიცხვითი ფიჩერებისთვის (numeric_pipeline):
ვავსებ დაკარგულ მნიშვნელობებს მედიანით (SimpleImputer(strategy='median')), რადგან მედიანა მდგრადია ექსტრემალურ მნიშვნელობებზე.
მას შემდეგ ვატარებ სტანდარტულ მასშტაბირებას (StandardScaler()), რომ ყველა რიცხვითი სვეტი მოექცეს საშუალო 0 და სტანდარტული გადახრა 1-ის გარშემო.
კატეგორიული ფიჩერებისთვის (categorical_pipeline):
ვავსებ დაკარგულ მნიშვნელობებს ყველაზე ხშირი მნიშვნელობით (SimpleImputer(strategy='most_frequent')).
შემდეგ ვაკეთებ Target Encoding-ს (TargetEncoder) category_encoders ბიბლიოთეკიდან:
ამ მეთოდისას კატეგორიული მნიშვნელობები ჩანაცვლდება მათი საშუალო მიზნობრივი ცვლადის (მაგალითად, ფრაუდის ალბათობის) მნიშვნელობებით.
Target Encoding უკეთ მუშაობს მაღალი-კარდინალურობის (ბევრი უნიკალური მნიშვნელობის) კატეგორიული სვეტებისთვის.ყველა preprocessing ნაბიჯს ვაერთიანებ ColumnTransformer-ში:
ColumnTransformer უზრუნველყოფს, რომ:
რიცხვითი სვეტებისთვის გამოიყენოს numeric_pipeline
კატეგორიული სვეტებისთვის გამოიყენოს categorical_pipeline
ამით ვქმნი ერთიან და სისტემატურ preprocessing ბლოკს, რომლის გამოყენებაც შემიძლია ნებისმიერი მოდელის ტრენინგისა და ვალიდაციის პროცესში.
ანუ მოკლედ რომ ვთქვა, ჩემი Feature Engineering pipeline მოიცავს:უსარგებლო სვეტების წაშლას (null 80%+). ნარჩენი null-ების გონივრულ შევსებას. Feature Scaling-ს რიცხვითი სვეტებისთვის. Target Encoding-ს კატეგორიული სვეტებისთვის.


Feature Selection:
Feature Selection ეტაპი ჩემს ნოუთბუქში შემდეგნაირად არის აწყობილი:
მონაცემების გაწმენდისა და საწყისი feature engineering-ის შემდეგ, გადავდივარ ფიჩერების შერჩევის (Feature Selection) ფაზაზე, სადაც მიზანია გამოვყო მნიშვნელოვანი სვეტები და მოვაშორო უსარგებლო ან ზედმეტად გადაკვეთი სვეტები.
ამ ეტაპზე ვიყენებ ორ სპეციალურად შექმნილ კლასს:
1. VarianceFilter: დაბალი დისპერსიის მქონე სვეტების წაშლა
პირველ რიგში, ვატარებ დაბალი დისპერსიის ფიჩერების მოცილებას VarianceFilter კლასის საშუალებით:
ვქმნი საკუთარ კლასს VarianceFilter, რომელიც ეფუძნება sklearn-ის VarianceThreshold მეთოდს.
ვმუშაობ მხოლოდ რიცხვით სვეტებზე (numeric columns).
ვაყენებ თრეშჰოლდს 0.01-ზე:
თუ სვეტის დისპერსია 0.01-ზე ნაკლებია, ეს ნიშნავს, რომ სვეტი თითქმის კონსტანტურია (ძალიან მცირე მერყეობით), ამიტომ ვთვლი უსარგებლოდ და ვშლი.
ტრანსფორმაციისას:
ვტოვებ მხოლოდ იმ რიცხვით სვეტებს, რომლებიც გადალახავენ დისპერსიის ზღვარს.
კატეგორიული სვეტები უცვლელად გადმომაქვს და ვაერთიანებ შერჩეულ რიცხვით სვეტებთან.
ამით ვცდილობ მონაცემებში noise-ის შემცირებას და მოდელის გამარტივებას.
2. CorrelationFilter: ძალიან კორელირებული სვეტების მოცილება
შემდეგ ეტაპზე, ვატარებ მაღალი კორელაციის მქონე ფიჩერების მოცილებას CorrelationFilter კლასის საშუალებით:
ვმუშაობ კვლავ მხოლოდ რიცხვით სვეტებზე.
ვშლი სვეტებს, რომლებიც:
ან სრული NaN-ია (Drop columns with all NaN).
ან აქვთ ნულოვანი დისპერსია (Drop constant columns).
შემდეგ ვანგარიშობ კორელაციის მატრიცას სვეტებს შორის.
ვიყენებ კორელაციის ზედა სამკუთხედ ნაწილს (upper triangle), რომ არ გავიმეორო წყვილები.
თუკი ორ სვეტს შორის კორელაცია მეტია ან ტოლია 0.95-ს (threshold=0.95):
ერთ-ერთი სვეტი ვშლის, რათა ავიცილო გადაჭარბებული ინფორმაციული ზედმეტობა და მოდელის გადატვირთვა.
რატომ ვაკეთებ ამას:
ძალიან მაღალი კორელაციის მქონე სვეტები ხშირ შემთხვევაში ერთმანეთს იმეორებენ და მოდელს შეიძლება დააბნიონ ან დააქვეითონ ზოგადი მონაცემების გენერალიზაციის უნარი.
3. პროცესი მთლიანობაში
მთლიანი Feature Selection პროცესში:
ჯერ ვაშორებ ძალიან დაბალი ვარიაციის სვეტებს (VarianceFilter).
შემდეგ ვაშორებ ძალიან ძლიერ კორელირებულ სვეტებს (CorrelationFilter).
ამ ორი საფეხურის გამოყენებით ვიღებ უფრო სუფთა, კომპაქტურ და ინფორმაციულ ფიჩერების ნაკრებს, რომელიც მზად არის მოდელზე სასწავლებლად.
შეჯამება:
მოკლედ, ჩემი Feature Selection pipeline მოიცავს:
VarianceThreshold-ზე დაფუძნებულ დაბალი დისპერსიის სვეტების მოცილებას.
95%-ზე მაღალი კორელაციის სვეტების მოცილებას.
ამით მნიშვნელოვნად ვამცირებ ფიჩერების ზედმეტობას და ვზრდი მოდელის ეფექტიანობას.



Training:Feature Engineering-ისა და Feature Selection-ის დასრულების შემდეგ გადავდივარ მოდელის გაწვრთნის ფაზაზე. ამ ეტაპზე ჩემი მიზანია გაწვრთნილი მოდელი სწორად მოახდინოს თაღლითური ტრანზაქციების პროგნოზირება ახალი მონაცემებისთვის.
პირველ რიგში, ვყოფ მთლიან მონაცემებს სასწავლო (train) და ვალიდაციის (validation) სეტებად შემდეგი სტრატეგიით:
გამოვიყენე train_test_split მეთოდი sklearn ბიბლიოთეკიდან.
ვაყენებ 80/20 გაყოფას: 80% მონაცემები მოდელის გასაწვრთნელად, 20% — სავალიდაციოდ.
ვიყენებ stratify=y პარამეტრს, რაც უზრუნველყოფს რომ ორივე სეტში კლასების განაწილება იგივე დარჩეს როგორც საწყის მონაცემებში. ეს განსაკუთრებით მნიშვნელოვანია გაუწონასწორებელ კლასებთან მუშაობისას, როგორიცაა თაღლითური ტრანზაქციები.
ვაყენებ random_state=42, რომ მონაცემების გაყოფა განმეორებადი იყოს და ექსპერიმენტების შედეგები შედარებადი.
შემდეგ ვაწყობ და ვატრენერებ Pipeline-ს:
Pipeline-ში გაერთიანებულია: წინასწარი დამუშავება (Preprocessing), Feature Selection და ძირითადი მოდელი.
Pipeline-ს ვატრენერებ მხოლოდ X_train და y_train სეტზე, რათა თავიდან ავიცილო Data Leakage და სწორად შევაფასო მოდელი ახალი მონაცემების პროგნოზირების უნარზე.
ტრენინგის დასრულების შემდეგ, ვაგენერირებ პროგნოზებს ვალიდაციის სეტზე:
ვგეგმავ predict_proba მეთოდის გამოყენებით, რათა მივიღო ტრანზაქციის თაღლითური კლასის ალბათობა.
ვიღებ მხოლოდ მეორე სვეტს [:, 1], რადგან ეს შეესაბამება პოზიტიურ კლასს (Fraud=True).
მოდელის შეფასებას ვახორციელებ ROC AUC მეტრიკით:
ვიყენებ roc_auc_score ფუნქციას sklearn ბიბლიოთეკიდან.
ROC AUC არის უმთავრესი მეტრიკა თაღლითობის გამოვლენის ამოცანებში, რადგან იგი ზომავს მოდელის უნარს სწორი განასხვავოს დადებითი და უარყოფითი კლასები სხვადასხვა თრეშჰოლდზე.
მთლიანი Training პროცესის მოკლე აღწერა:
მონაცემების სტრატიფიცირებული გაყოფა.
Pipeline-ის გაწვრთნა მხოლოდ სასწავლო სეტზე.
პროგნოზირება ვალიდაციის სეტზე.
შეფასება ROC AUC სკორით.

MLflow Tracking:
XGBClassifierr: https://dagshub.com/agasi22/ml-2.mlflow/#/experiments/1?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D
Random Forest: https://dagshub.com/agasi22/ml-2.mlflow/#/experiments/2?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D
LogisticRegression: https://dagshub.com/agasi22/ml-2.mlflow/#/experiments/3?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D
AdaBoost: https://dagshub.com/agasi22/ml-2.mlflow/#/experiments/4?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D
მოდელებში ვცვლიდი ფიჩერების ამორცევის მეთოდებსა და ჰიპერპარამეტრებს. საბოლოდ ყველაზე კარგი შედეგის მქონე და ეფექტური მოდელები შენახულია Model Registry - ში. ვლოგავდი მოდელების სხვადასხვა ჰიპერპარამეტრებს და auc ს თიტოეული ექპერიმენტისას რომ მქონოდა წარმოდგენა მოდელის ეფექტურობაზე. ჩემი საუკეთესო მოდელია https://dagshub.com/agasi22/ml-2.mlflow/#/experiments/1/runs/efa27b1225a243df965b8f3fb7a69936 და მისი შედეგი საბმიშენის გაკეთების შემდეგ იყო  Score: 0.928239 Private score: 0.890120.
